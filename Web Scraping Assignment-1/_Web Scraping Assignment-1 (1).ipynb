{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d913d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header Tags\n",
      "0  Wikipedia\\n\\nThe Free Encyclopedia\n",
      "1            1 000 000+\\n\\n\\narticles\n",
      "2              100 000+\\n\\n\\narticles\n",
      "3               10 000+\\n\\n\\narticles\n",
      "4                1 000+\\n\\n\\narticles\n",
      "5                  100+\\n\\n\\narticles\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.wikipedia.org/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "header_text = [header.text.strip() for header in headers]\n",
    "\n",
    "df = pd.DataFrame({'Header Tags': header_text})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98880aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table', class_='tablepress')\n",
    "\n",
    "names = []\n",
    "terms = []\n",
    "\n",
    "for row in table.find_all('tr'):\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) >= 2:\n",
    "        names.append(columns[0].text.strip())\n",
    "        terms.append(columns[1].text.strip())\n",
    "\n",
    "data = {'Name': names, 'Term of Office': terms}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_rankings(url, columns):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "   \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:11]: \n",
    "       \n",
    "        values = [cell.text.strip() for cell in row.find_all('td')]\n",
    "        data.append(values)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "team_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "team_columns = ['Position', 'Team', 'Matches', 'Points', 'Rating']\n",
    "team_df = scrape_rankings(team_url, team_columns)\n",
    "\n",
    "batsman_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "batsman_columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "batsman_df = scrape_rankings(batsman_url, batsman_columns)\n",
    "\n",
    "bowler_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "bowler_columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "bowler_df = scrape_rankings(bowler_url, bowler_columns)\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(team_df)\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(batsman_df)\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(bowler_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_rankings(url, columns):\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:11]:  \n",
    "        values = [cell.text.strip() for cell in row.find_all('td')]\n",
    "        data.append(values)\n",
    "\n",
    "   \n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "team_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "team_columns = ['Position', 'Team', 'Matches', 'Points', 'Rating']\n",
    "team_df = scrape_rankings(team_url, team_columns)\n",
    "\n",
    "batting_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "batting_columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "batting_df = scrape_rankings(batting_url, batting_columns)\n",
    "\n",
    "allrounder_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "allrounder_columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "allrounder_df = scrape_rankings(allrounder_url, allrounder_columns)\n",
    "\n",
    "print(\"Top 10 ODI Teams in Women's Cricket:\")\n",
    "print(team_df)\n",
    "print(\"\\nTop 10 ODI Batting Players in Women's Cricket:\")\n",
    "print(batting_df)\n",
    "print(\"\\nTop 10 ODI All-rounders in Women's Cricket:\")\n",
    "print(allrounder_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28995148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "container = soup.find('div', class_='Card-titleContainer')\n",
    "\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "for article in container.find_all('div', class_='Card-title'):\n",
    "    headline = article.find('a').text.strip()\n",
    "    time = article.find('time').text.strip()\n",
    "    link = article.find('a')['href']\n",
    "    \n",
    "    headlines.append(headline)\n",
    "    times.append(time)\n",
    "    links.append(link)\n",
    "\n",
    "data = {'Headline': headlines, 'Time': times, 'News Link': links}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "container = soup.find('ul', class_='article-list')\n",
    "\n",
    "\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "for article in container.find_all('li'):\n",
    "    title = article.find('a', class_='article-title').text.strip()\n",
    "    author = article.find('ul', class_='author-list').text.strip()\n",
    "    date = article.find('span', class_='date-text').text.strip()\n",
    "    url = article.find('a', class_='article-title')['href']\n",
    "    \n",
    "    titles.append(title)\n",
    "    authors.append(author)\n",
    "    dates.append(date)\n",
    "    urls.append(url)\n",
    "\n",
    "data = {'Paper Title': titles, 'Authors': authors, 'Published Date': dates, 'Paper URL': urls}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "container = soup.find_all('div', class_='restnt-info')\n",
    "\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "for item in container:\n",
    "    name = item.find('a', class_='restnt-name ellipsis').text.strip()\n",
    "    cuisine = item.find('span', class_='double-line-ellipsis').text.strip()\n",
    "    location = item.find('span', class_='locality').text.strip()\n",
    "    rating = item.find('span', class_='rat-star').text.strip()\n",
    "    image_url = item.find('img')['data-src']\n",
    "\n",
    "    restaurant_names.append(name)\n",
    "    cuisines.append(cuisine)\n",
    "    locations.append(location)\n",
    "    ratings.append(rating)\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "data = {\n",
    "    'Restaurant Name': restaurant_names,\n",
    "    'Cuisine': cuisines,\n",
    "    'Location': locations,\n",
    "    'Ratings': ratings,\n",
    "    'Image URL': image_urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
