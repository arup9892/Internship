{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1ba566",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n",
    "\n",
    "2.In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edfde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def search_amazon_products(product_name):\n",
    "    base_url = 'https://www.amazon.in'\n",
    "    search_url = f'{base_url}/s?k={product_name.replace(\" \", \"+\")}'\n",
    "\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    products = []\n",
    "    pages = min(3, int(soup.select('.s-pagination li')[-2].text))  # Get the number of pages (maximum of 3)\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        page_url = f'{search_url}&page={page}'\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        product_cards = soup.select('.s-result-item')\n",
    "\n",
    "        for card in product_cards:\n",
    "            brand_name = card.select_one('.s-line-clamp-1').get_text(strip=True)\n",
    "            product_name = card.select_one('.a-text-normal').get_text(strip=True)\n",
    "            price = card.select_one('.a-price-whole')\n",
    "            return_exchange = card.select_one('.s-replace-4')\n",
    "            expected_delivery = card.select_one('.s-replace-5')\n",
    "            availability = card.select_one('.a-color-state')\n",
    "            product_url = base_url + card.select_one('.a-link-normal')['href']\n",
    "\n",
    "            price = price.get_text(strip=True) if price else '-'\n",
    "            return_exchange = return_exchange.get_text(strip=True) if return_exchange else '-'\n",
    "            expected_delivery = expected_delivery.get_text(strip=True) if expected_delivery else '-'\n",
    "            availability = availability.get_text(strip=True) if availability else '-'\n",
    "\n",
    "            product = {\n",
    "                'Brand Name': brand_name,\n",
    "                'Product Name': product_name,\n",
    "                'Price': price,\n",
    "                'Return/Exchange': return_exchange,\n",
    "                'Expected Delivery': expected_delivery,\n",
    "                'Availability': availability,\n",
    "                'Product URL': product_url\n",
    "            }\n",
    "            products.append(product)\n",
    "\n",
    "    return products\n",
    "\n",
    "product = input('Enter the product name: ')\n",
    "\n",
    "results = search_amazon_products(product)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('amazon_products.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58aedc6",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    billionaires = []\n",
    "    table = soup.find('table')\n",
    "    rows = table.tbody.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        rank = cells[0].text.strip()\n",
    "        name = cells[1].text.strip()\n",
    "        net_worth = cells[2].text.strip()\n",
    "        age = cells[3].text.strip()\n",
    "        citizenship = cells[4].text.strip()\n",
    "        source = cells[5].text.strip()\n",
    "        industry = cells[6].text.strip()\n",
    "\n",
    "        billionaire = {\n",
    "            'Rank': rank,\n",
    "            'Name': name,\n",
    "            'Net Worth': net_worth,\n",
    "            'Age': age,\n",
    "            'Citizenship': citizenship,\n",
    "            'Source': source,\n",
    "            'Industry': industry\n",
    "        }\n",
    "        billionaires.append(billionaire)\n",
    "\n",
    "    return billionaires\n",
    "\n",
    "results = scrape_forbes_billionaires()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('forbes_billionaires.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadb706",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c369140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    url = f\"https://www.google.com/search?q={keyword}&tbm=isch\"\n",
    "\n",
    "   \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    images = []\n",
    "    count = 0\n",
    "\n",
    "    \n",
    "    image_elements = soup.select('.rg_i.Q4LuWd')\n",
    "\n",
    "    for image_element in image_elements:\n",
    "        if count >= num_images:\n",
    "            break\n",
    "\n",
    "    \n",
    "        image_url = image_element['data-src']\n",
    "\n",
    "        if image_url is not None and image_url.startswith('http'):\n",
    "            try:\n",
    "               \n",
    "                urllib.request.urlretrieve(image_url, f\"{keyword}_{count+1}.jpg\")\n",
    "                images.append(image_url)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping image: {e}\")\n",
    "\n",
    "    return images\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images = 10\n",
    "\n",
    "results = {}\n",
    "for keyword in keywords:\n",
    "    images = scrape_images(keyword, num_images)\n",
    "    results[keyword] = images\n",
    "\n",
    "for keyword, images in results.items():\n",
    "    print(f\"Images for keyword '{keyword}':\")\n",
    "    for i, image_url in enumerate(images):\n",
    "        print(f\"Image {i+1}: {image_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0944fc5",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def search_flipkart_smartphones(product_name):\n",
    "    base_url = 'https://www.flipkart.com'\n",
    "    search_url = f'{base_url}/search?q={product_name.replace(\" \", \"+\")}'\n",
    "\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    smartphones = []\n",
    "    product_cards = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "    for card in product_cards:\n",
    "        brand_name = card.find('div', {'class': '_4rR01T'})\n",
    "        product_name = card.find('a', {'class': '_1fQZEK'})\n",
    "        colour = card.find('a', {'class': '_1fQZEK'}).get('title')\n",
    "        ram = card.find('ul', {'class': '_1xgFaf'}).li\n",
    "        storage = card.find('ul', {'class': '_1xgFaf'}).find_all('li')[1]\n",
    "        primary_camera = card.find('ul', {'class': '_1xgFaf'}).find_all('li')[2]\n",
    "        secondary_camera = card.find('ul', {'class': '_1xgFaf'}).find_all('li')[3]\n",
    "        display_size = card.find('ul', {'class': '_1xgFaf'}).find_all('li')[4]\n",
    "        battery_capacity = card.find('ul', {'class': '_1xgFaf'}).find_all('li')[5]\n",
    "        price = card.find('div', {'class': '_30jeq3 _1_WHN1'})\n",
    "        product_url = base_url + card.find('a', {'class': '_1fQZEK'}).get('href')\n",
    "\n",
    "        # If any of the details is missing, replace it with '-'\n",
    "        brand_name = brand_name.get_text(strip=True) if brand_name else '-'\n",
    "        product_name = product_name.get_text(strip=True) if product_name else '-'\n",
    "        ram = ram.get_text(strip=True) if ram else '-'\n",
    "        storage = storage.get_text(strip=True) if storage else '-'\n",
    "        primary_camera = primary_camera.get_text(strip=True) if primary_camera else '-'\n",
    "        secondary_camera = secondary_camera.get_text(strip=True) if secondary_camera else '-'\n",
    "        display_size = display_size.get_text(strip=True) if display_size else '-'\n",
    "        battery_capacity = battery_capacity.get_text(strip=True) if battery_capacity else '-'\n",
    "        price = price.get_text(strip=True) if price else '-'\n",
    "\n",
    "        smartphone = {\n",
    "            'Brand Name': brand_name,\n",
    "            'Smartphone Name': product_name,\n",
    "            'Colour': colour,\n",
    "            'RAM': ram,\n",
    "            'Storage(ROM)': storage,\n",
    "            'Primary Camera': primary_camera,\n",
    "            'Secondary Camera': secondary_camera,\n",
    "            'Display Size': display_size,\n",
    "            'Battery Capacity': battery_capacity,\n",
    "            'Price': price,\n",
    "            'Product URL': product_url\n",
    "        }\n",
    "        smartphones.append(smartphone)\n",
    "\n",
    "    return smartphones\n",
    "\n",
    "\n",
    "product = input('Enter the smartphone name: ')\n",
    "\n",
    "results = search_flipkart_smartphones(product)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('flipkart_smartphones.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3a660",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_coordinates(city):\n",
    "    geolocator = Nominatim(user_agent='my_app')\n",
    "    location = geolocator.geocode(city)\n",
    "\n",
    "    if location:\n",
    "        latitude = location.latitude\n",
    "        longitude = location.longitude\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "city = input('Enter the city name: ')\n",
    "\n",
    "latitude, longitude = get_coordinates(city)\n",
    "\n",
    "if latitude is not None and longitude is not None:\n",
    "    print(f\"Coordinates for {city}: Latitude = {latitude}, Longitude = {longitude}\")\n",
    "else:\n",
    "    print(\"Unable to find coordinates for the specified city.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec380531",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    laptops = []\n",
    "    laptop_cards = soup.find_all('div', {'class': 'TopNumbeHeading sticky-footer'})\n",
    "\n",
    "    for card in laptop_cards:\n",
    "        laptop_name = card.find('div', {'class': 'TopNumbeListing'}).get_text(strip=True)\n",
    "        specs = card.find_next_sibling('ul')\n",
    "\n",
    "        specifications = {}\n",
    "        for spec in specs.find_all('li'):\n",
    "            spec_name = spec.find('div', {'class': 'Specs-Wrap'}).get_text(strip=True)\n",
    "            spec_value = spec.find('div', {'class': 'Specs-Details'}).get_text(strip=True)\n",
    "            specifications[spec_name] = spec_value\n",
    "\n",
    "        laptop = {\n",
    "            'Laptop Name': laptop_name,\n",
    "            **specifications\n",
    "        }\n",
    "        laptops.append(laptop)\n",
    "\n",
    "    return laptops\n",
    "\n",
    "\n",
    "results = scrape_gaming_laptops()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('gaming_laptops.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c76808",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8891b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels():\n",
    "    url = 'https://www.hostelworld.com/search?location=London&country=England'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    hostels = []\n",
    "    hostel_cards = soup.find_all('div', {'class': 'fabresult'})\n",
    "\n",
    "    for card in hostel_cards:\n",
    "        hostel_name = card.find('h2', {'class': 'title'}).get_text(strip=True)\n",
    "        distance = card.find('span', {'class': 'distance'}).get_text(strip=True)\n",
    "        ratings = card.find('div', {'class': 'rating'}).get_text(strip=True)\n",
    "        total_reviews = card.find('div', {'class': 'reviews'}).get_text(strip=True)\n",
    "        overall_reviews = card.find('div', {'class': 'score orange'}).get_text(strip=True)\n",
    "        privates_from_price = card.find('div', {'class': 'price'}).get_text(strip=True)\n",
    "        dorms_from_price = card.find('div', {'class': 'price'}).find_next('div').get_text(strip=True)\n",
    "        facilities = ', '.join([f.get_text(strip=True) for f in card.find_all('span', {'class': 'facilities-label'})])\n",
    "        property_description = card.find('div', {'class': 'desc'}).get_text(strip=True)\n",
    "\n",
    "        hostel = {\n",
    "            'Hostel Name': hostel_name,\n",
    "            'Distance from City Centre': distance,\n",
    "            'Ratings': ratings,\n",
    "            'Total Reviews': total_reviews,\n",
    "            'Overall Reviews': overall_reviews,\n",
    "            'Privates from Price': privates_from_price,\n",
    "            'Dorms from Price': dorms_from_price,\n",
    "            'Facilities': facilities,\n",
    "            'Property Description': property_description\n",
    "        }\n",
    "        hostels.append(hostel)\n",
    "\n",
    "    return hostels\n",
    "\n",
    "results = scrape_hostels()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('hostels_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
