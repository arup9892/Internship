{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665b10a5",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "rows = table.find_all(\"tr\")[1:]  # Exclude the header row\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip()\n",
    "\n",
    "    print(f\"Rank: {rank}\")\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Artist: {artist}\")\n",
    "    print(f\"Upload Date: {upload_date}\")\n",
    "    print(f\"Views: {views}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a913973",
   "metadata": {},
   "source": [
    "2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://www.bcci.tv\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "fixture_link = soup.find(\"a\", class_=\"navigation__link navigation__link--in-drop-down\")\n",
    "fixture_url = base_url + fixture_link[\"href\"]\n",
    "\n",
    "response = requests.get(fixture_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "fixtures = soup.find_all(\"div\", class_=\"fixture__details\")\n",
    "\n",
    "for fixture in fixtures:\n",
    "    title = fixture.find(\"div\", class_=\"fixture__description\").text.strip()\n",
    "    series = fixture.find(\"span\", class_=\"u-unskewed-text\").text.strip()\n",
    "    place = fixture.find(\"p\", class_=\"fixture__additional-info\").text.strip()\n",
    "    date = fixture.find(\"span\", class_=\"fixture__date\").text.strip()\n",
    "    time = fixture.find(\"span\", class_=\"fixture__time\").text.strip()\n",
    "\n",
    "    print(f\"Match Title: {title}\")\n",
    "    print(f\"Series: {series}\")\n",
    "    print(f\"Place: {place}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Time: {time}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a32f2f",
   "metadata": {},
   "source": [
    "Q3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acdb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "base_url = \"http://statisticstimes.com/\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "economy_link = soup.find(\"div\", class_=\"navbar\").find_all(\"a\")[2]\n",
    "economy_url = base_url + economy_link[\"href\"]\n",
    "\n",
    "response = requests.get(economy_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_=\"display dataTable\")\n",
    "rows = table.find_all(\"tr\")[1:]  # Exclude the header row\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gdp_18_19 = columns[2].text.strip()\n",
    "    gdp_19_20 = columns[3].text.strip()\n",
    "    share_18_19 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "\n",
    "    print(f\"Rank: {rank}\")\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"GSDP (18-19) - at current prices: {gdp_18_19}\")\n",
    "    print(f\"GSDP (19-20) - at current prices: {gdp_19_20}\")\n",
    "    print(f\"Share (18-19): {share_18_19}\")\n",
    "    print(f\"GDP ($ billion): {gdp_billion}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30178f6",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "base_url = \"https://github.com\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "explore_menu = soup.find(\"summary\", string=\"Explore\")\n",
    "trending_link = explore_menu.find_next_sibling(\"details-menu\").find(\"a\", string=\"Trending\")\n",
    "trending_url = base_url + trending_link[\"href\"]\n",
    "\n",
    "response = requests.get(trending_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "repositories = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "\n",
    "for repo in repositories:\n",
    "    title = repo.find(\"h1\", class_=\"h3\").text.strip()\n",
    "    description = repo.find(\"p\", class_=\"col-9\").text.strip()\n",
    "    contributors = repo.find(\"a\", href=lambda href: href and \"graphs/contributors\" in href).find(\"span\").text.strip()\n",
    "    language = repo.find(\"span\", itemprop=\"programmingLanguage\").text.strip()\n",
    "\n",
    "    print(f\"Repository Title: {title}\")\n",
    "    print(f\"Repository Description: {description}\")\n",
    "    print(f\"Contributors Count: {contributors}\")\n",
    "    print(f\"Language Used: {language}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cd7bf",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "base_url = \"https://www.billboard.com\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "charts_link = soup.find(\"a\", text=\"Charts\")\n",
    "hot100_link = base_url + charts_link[\"href\"]\n",
    "\n",
    "response = requests.get(hot100_link)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "songs = soup.find_all(\"li\", class_=\"chart-list__element\")\n",
    "\n",
    "for song in songs:\n",
    "    name = song.find(\"span\", class_=\"chart-element__information__song\").text.strip()\n",
    "    artist = song.find(\"span\", class_=\"chart-element__information__artist\").text.strip()\n",
    "    last_week_rank = song.find(\"span\", class_=\"chart-element__meta text--last\").text.strip()\n",
    "    peak_rank = song.find(\"span\", class_=\"chart-element__meta text--peak\").text.strip()\n",
    "    weeks_on_board = song.find(\"span\", class_=\"chart-element__meta text--week\").text.strip()\n",
    "\n",
    "    print(f\"Song Name: {name}\")\n",
    "    print(f\"Artist Name: {artist}\")\n",
    "    print(f\"Last Week Rank: {last_week_rank}\")\n",
    "    print(f\"Peak Rank: {peak_rank}\")\n",
    "    print(f\"Weeks on Board: {weeks_on_board}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ef4c7",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest sellingnovels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_=\"in-article sortable\")\n",
    "rows = table.find_all(\"tr\")[1:]  # Exclude the header row\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    book_name = columns[1].text.strip()\n",
    "    author_name = columns[2].text.strip()\n",
    "    volumes_sold = columns[3].text.strip()\n",
    "    publisher = columns[4].text.strip()\n",
    "    genre = columns[5].text.strip()\n",
    "\n",
    "    print(f\"Book Name: {book_name}\")\n",
    "    print(f\"Author Name: {author_name}\")\n",
    "    print(f\"Volumes Sold: {volumes_sold}\")\n",
    "    print(f\"Publisher: {publisher}\")\n",
    "    print(f\"Genre: {genre}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeab137",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18145bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "series_list = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "for series in series_list:\n",
    "    name = series.find(\"h3\").find(\"a\").text.strip()\n",
    "    year_span = series.find(\"span\", class_=\"lister-item-year\").text.strip(\"()\").strip()\n",
    "    genre = series.find(\"span\", class_=\"genre\").text.strip()\n",
    "    run_time = series.find(\"span\", class_=\"runtime\").text.strip()\n",
    "    rating = series.find(\"span\", class_=\"ipl-rating-star__rating\").text.strip()\n",
    "    votes = series.find(\"span\", attrs={\"name\": \"nv\"}).text.strip().replace(\",\", \"\")\n",
    "\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Year Span: {year_span}\")\n",
    "    print(f\"Genre: {genre}\")\n",
    "    print(f\"Run Time: {run_time}\")\n",
    "    print(f\"Ratings: {rating}\")\n",
    "    print(f\"Votes: {votes}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de48453",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4934a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "base_url = \"https://archive.ics.uci.edu\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "show_all_link = soup.find(\"a\", text=\"Show All Dataset\")\n",
    "show_all_url = base_url + show_all_link[\"href\"]\n",
    "\n",
    "response = requests.get(show_all_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "datasets = soup.find_all(\"table\")[5].find_all(\"tr\")[1:]  # Exclude the header row\n",
    "\n",
    "for dataset in datasets:\n",
    "    columns = dataset.find_all(\"td\")\n",
    "    dataset_name = columns[0].text.strip()\n",
    "    data_type = columns[1].text.strip()\n",
    "    task = columns[2].text.strip()\n",
    "    attribute_type = columns[3].text.strip()\n",
    "    instances = columns[4].text.strip()\n",
    "    attributes = columns[5].text.strip()\n",
    "    year = columns[6].text.strip()\n",
    "\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(f\"Data Type: {data_type}\")\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Attribute Type: {attribute_type}\")\n",
    "    print(f\"No. of Instances: {instances}\")\n",
    "    print(f\"No. of Attributes: {attributes}\")\n",
    "    print(f\"Year: {year}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4eb4cd",
   "metadata": {},
   "source": [
    "9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants \n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company\n",
    "D)Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://www.naukri.com\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "recruiters_link = soup.find(\"a\", text=\"Recruiters\")\n",
    "recruiters_url = base_url + recruiters_link[\"href\"]\n",
    "\n",
    "response = requests.get(recruiters_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "search_box = soup.find(\"input\", {\"id\": \"root_keywords\"})\n",
    "search_box[\"value\"] = \"Data Science\"\n",
    "\n",
    "search_button = soup.find(\"button\", {\"id\": \"root_sbtBtn\"})\n",
    "search_form = search_button.find_parent(\"form\")\n",
    "search_action = search_form[\"action\"]\n",
    "search_url = base_url + search_action\n",
    "\n",
    "response = requests.post(search_url, data=search_form)\n",
    "\n",
    "\n",
    "recruiters = soup.find_all(\"article\", class_=\"row\")\n",
    "\n",
    "for recruiter in recruiters:\n",
    "    name = recruiter.find(\"a\", class_=\"rec_name\").text.strip()\n",
    "    designation = recruiter.find(\"span\", class_=\"rec_designation\").text.strip()\n",
    "    company = recruiter.find(\"a\", class_=\"rec_company\").text.strip()\n",
    "    skills = recruiter.find(\"div\", class_=\"rec_skills\").text.strip()\n",
    "    location = recruiter.find(\"span\", class_=\"rec_location\").text.strip()\n",
    "\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Designation: {designation}\")\n",
    "    print(f\"Company: {company}\")\n",
    "    print(f\"Skills They Hire For: {skills}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(\"------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
