{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7396083",
   "metadata": {},
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name and Price) from https://www.motor1.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99745e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.motor1.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "search_bar = soup.find('input', {'class': 'search-input'})\n",
    "search_bar['value'] = '50 most expensive cars'\n",
    "\n",
    "search_button = soup.find('button', {'class': 'search-submit'})\n",
    "search_results_page = requests.get(url + search_button['data-action'])\n",
    "search_results_soup = BeautifulSoup(search_results_page.text, 'html.parser')\n",
    "\n",
    "car_list = []\n",
    "price_list = []\n",
    "\n",
    "car_items = search_results_soup.find_all('div', {'class': 'card-group__item'})\n",
    "for car_item in car_items:\n",
    "    car_name = car_item.find('h3').text.strip()\n",
    "    car_price = car_item.find('span', {'class': 'card-group__price'}).text.strip()\n",
    "\n",
    "    car_list.append(car_name)\n",
    "    price_list.append(car_price)\n",
    "\n",
    "data = {'Car Name': car_list, 'Price': price_list}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcbaed7",
   "metadata": {},
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63754841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.jagranjosh.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "gk_option = soup.find('a', text='GK')\n",
    "gk_url = url + gk_option['href']\n",
    "gk_response = requests.get(gk_url)\n",
    "gk_soup = BeautifulSoup(gk_response.text, 'html.parser')\n",
    "\n",
    "\n",
    "pm_option = gk_soup.find('a', text='List of all Prime Ministers of India')\n",
    "pm_url = url + pm_option['href']\n",
    "pm_response = requests.get(pm_url)\n",
    "pm_soup = BeautifulSoup(pm_response.text, 'html.parser')\n",
    "\n",
    "prime_ministers = []\n",
    "born_dead = []\n",
    "term_of_office = []\n",
    "remarks = []\n",
    "\n",
    "table = pm_soup.find('table')\n",
    "rows = table.find_all('tr')[1:]  \n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    prime_minister = cells[0].text.strip()\n",
    "    prime_ministers.append(prime_minister)\n",
    "\n",
    "    details = cells[1].text.strip().split(' - ')\n",
    "    born_dead.append(details[0])\n",
    "    term_of_office.append(details[1])\n",
    "    remarks.append(cells[2].text.strip())\n",
    "\n",
    "data = {\n",
    "    'Name': prime_ministers,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': term_of_office,\n",
    "    'Remarks': remarks\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33480d59",
   "metadata": {},
   "source": [
    "Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpage https://www.azquotes.com/\n",
    "2. Click on Top Quotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b24b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.jagranjosh.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "gk_option = soup.find('a', text='GK')\n",
    "gk_url = url + gk_option['href']\n",
    "gk_response = requests.get(gk_url)\n",
    "gk_soup = BeautifulSoup(gk_response.text, 'html.parser')\n",
    "\n",
    "pm_option = gk_soup.find('a', text='List of all Prime Ministers of India')\n",
    "pm_url = url + pm_option['href']\n",
    "pm_response = requests.get(pm_url)\n",
    "pm_soup = BeautifulSoup(pm_response.text, 'html.parser')\n",
    "\n",
    "\n",
    "prime_ministers = []\n",
    "born_dead = []\n",
    "term_of_office = []\n",
    "remarks = []\n",
    "\n",
    "table = pm_soup.find('table')\n",
    "rows = table.find_all('tr')[1:]  \n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    prime_minister = cells[0].text.strip()\n",
    "    prime_ministers.append(prime_minister)\n",
    "\n",
    "    details = cells[1].text.strip().split(' - ')\n",
    "    born_dead.append(details[0])\n",
    "    term_of_office.append(details[1])\n",
    "    remarks.append(cells[2].text.strip())\n",
    "\n",
    "data = {\n",
    "    'Name': prime_ministers,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': term_of_office,\n",
    "    'Remarks': remarks\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84002a37",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bfd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.shine.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "search_field = soup.find('input', {'id': 'txt_search'})\n",
    "search_field['value'] = 'Data Analyst'\n",
    "\n",
    "location_field = soup.find('input', {'id': 'txtLocation'})\n",
    "location_field['value'] = 'Bangalore'\n",
    "\n",
    "search_button = soup.find('button', {'id': 'btnSearch'})\n",
    "search_results_page = requests.post(url, data={'txt_search': 'Data Analyst', 'txtLocation': 'Bangalore'})\n",
    "\n",
    "job_list = []\n",
    "location_list = []\n",
    "company_list = []\n",
    "experience_list = []\n",
    "\n",
    "search_results_soup = BeautifulSoup(search_results_page.text, 'html.parser')\n",
    "jobs = search_results_soup.find_all('a', {'class': 'sjs-link'})\n",
    "\n",
    "for job in jobs[:10]:\n",
    "    job_title = job.find('h3').text.strip()\n",
    "    job_location = job.find('span', {'class': 'location'}).text.strip()\n",
    "    company_name = job.find('span', {'class': 'company'}).text.strip()\n",
    "    experience_required = job.find('span', {'class': 'exp'}).text.strip()\n",
    "\n",
    "    job_list.append(job_title)\n",
    "    location_list.append(job_location)\n",
    "    company_list.append(company_name)\n",
    "    experience_list.append(experience_required)\n",
    "\n",
    "data = {\n",
    "    'Job Title': job_list,\n",
    "    'Job Location': location_list,\n",
    "    'Company Name': company_list,\n",
    "    'Experience Required': experience_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d62dc",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e946d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.shine.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "search_field = soup.find('input', {'id': 'txt_search'})\n",
    "search_field['value'] = 'Data Scientist'\n",
    "\n",
    "location_field = soup.find('input', {'id': 'txtLocation'})\n",
    "location_field['value'] = 'Bangalore'\n",
    "\n",
    "search_button = soup.find('button', {'id': 'btnSearch'})\n",
    "search_results_page = requests.post(url, data={'txt_search': 'Data Scientist', 'txtLocation': 'Bangalore'})\n",
    "\n",
    "job_list = []\n",
    "location_list = []\n",
    "company_list = []\n",
    "\n",
    "search_results_soup = BeautifulSoup(search_results_page.text, 'html.parser')\n",
    "jobs = search_results_soup.find_all('a', {'class': 'sjs-link'})\n",
    "\n",
    "for job in jobs[:10]:\n",
    "    job_title = job.find('h3').text.strip()\n",
    "    job_location = job.find('span', {'class': 'location'}).text.strip()\n",
    "    company_name = job.find('span', {'class': 'company'}).text.strip()\n",
    "\n",
    "    job_list.append(job_title)\n",
    "    location_list.append(job_location)\n",
    "    company_list.append(company_name)\n",
    "\n",
    "data = {\n",
    "    'Job Title': job_list,\n",
    "    'Job Location': location_list,\n",
    "    'Company Name': company_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728362d5",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/\n",
    "itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22060bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "reviews = soup.find_all('div', {'class': '_27M-vq'})\n",
    "\n",
    "count = 0\n",
    "for review in reviews:\n",
    "    count += 1\n",
    "    rating = review.find('div', {'class': '_3LWZlK _1BLPMq'}).text.strip()\n",
    "    summary = review.find('p', {'class': '_2-N8zT'}).text.strip()\n",
    "    full_review = review.find('div', {'class': 't-ZTKy'}).div.div.text.strip()\n",
    "\n",
    "    print(f'Review {count}:')\n",
    "    print(f'Rating: {rating}')\n",
    "    print(f'Summary: {summary}')\n",
    "    print(f'Full Review: {full_review}\\n')\n",
    "\n",
    "    if count == 100:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
